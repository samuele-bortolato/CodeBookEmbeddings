{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, GlueDataTrainingArguments, AutoTokenizer\n",
    "import torchmetrics\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from apx import ApproxEmbed, ApxSVD, train_apx\n",
    "from glue_score import GLUE_TASKS, make_model, Glue, get_dataloaders, validate, get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.utils.logging.set_verbosity_error()\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "epochs=5\n",
    "batch_size=32\n",
    "\n",
    "lr=5e-5\n",
    "levels=8\n",
    "channels=8\n",
    "bits=8\n",
    "neurons=64\n",
    "nn_levels=2\n",
    "apx_epochs=5000\n",
    "apx_batch_size=2**14\n",
    "checkpoint_every=100\n",
    "\n",
    "save_path = 'results/compression_finetuned/'\n",
    "\n",
    "norms = [1,1.25,1.5,1.75,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mrpc'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task=list(GLUE_TASKS.keys())[2]\n",
    "args=GLUE_TASKS[task]\n",
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (C:/Users/Sam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (C:/Users/Sam/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Epoch: 1: 100%|██████████| 115/115 [00:03<00:00, 34.44it/s, loss: 0.164]\n",
      "Epoch: 2: 100%|██████████| 115/115 [00:02<00:00, 38.34it/s, loss: 0.156]\n",
      "Epoch: 3: 100%|██████████| 115/115 [00:02<00:00, 38.70it/s, loss: 0.154]\n",
      "Epoch: 4: 100%|██████████| 115/115 [00:03<00:00, 36.77it/s, loss: 0.152]\n",
      "Epoch: 5: 100%|██████████| 115/115 [00:03<00:00, 38.05it/s, loss: 0.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original [tensor(0.4950), tensor(0.5378)]\n"
     ]
    }
   ],
   "source": [
    "model=make_model('prajjwal1/bert-tiny',args)\n",
    "    \n",
    "train_dataloader, val_dataloader = get_dataloaders(args, task, batch_size)\n",
    "metrics = get_metrics(args)\n",
    "\n",
    "Glue(model, tokenizer, task, args, epochs=epochs, steps_validate=0.2, train_dataloader=train_dataloader, val_dataloader=val_dataloader);\n",
    "\n",
    "embeddings = model.bert.embeddings.word_embeddings.weight.to(device)\n",
    "\n",
    "results={}\n",
    "\n",
    "val_metrics = validate(model, tokenizer, val_dataloader, metrics, args)\n",
    "results['Original']=[m.item() for m in val_metrics]\n",
    "print('Original', val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999 0.0031031                                                                        0.10002279281616211\n",
      "2 [tensor(0.4061), tensor(0.5000)]\n"
     ]
    }
   ],
   "source": [
    "apx_model = make_model('prajjwal1/bert-tiny',args)\n",
    "apx_model.load_state_dict(model.state_dict())\n",
    "\n",
    "apx = ApproxEmbed(levels = levels, \n",
    "        feature_dim = channels,\n",
    "        num_words = embeddings.shape[0],\n",
    "        output_dims = embeddings.shape[1],\n",
    "        feature_std = 0.1,\n",
    "        feature_bias = 0.0,\n",
    "        codebook_bitwidth=bits,\n",
    "        neurons = neurons,\n",
    "        nn_levels=nn_levels).to('cuda')\n",
    "\n",
    "run = train_apx(apx, embeddings, apx_epochs, apx_batch_size, checkpoint_every, save_path, verbose=True);\n",
    "apx.fix_indices()\n",
    "apx_model.bert.embeddings.word_embeddings = apx\n",
    "\n",
    "val_metrics = validate(apx_model, tokenizer, val_dataloader, metrics, args)\n",
    "results[\"Norm \"+str(norm)]=[m.item() for m in val_metrics]\n",
    "print(norm, val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(len(run['loss']))+1,run['loss'])\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0025183973592193343\n"
     ]
    }
   ],
   "source": [
    "order=np.arange(embeddings.shape[0])\n",
    "tot_loss=0\n",
    "tot_n=0\n",
    "np.random.shuffle(order)\n",
    "for batch in range(int(np.ceil(len(order)/batch_size))):\n",
    "\n",
    "    positions = torch.tensor(order[batch*batch_size:(batch+1)*batch_size],dtype=int,device=device)\n",
    "    embed_preds = apx(positions)\n",
    "\n",
    "    embeds = embeddings[positions]\n",
    "    loss=torch.mean(torch.pow(torch.abs(embed_preds-embeds), norm))\n",
    "\n",
    "    tot_loss+=loss.item()*len(order[batch*batch_size:(batch+1)*batch_size])\n",
    "    tot_n+=len(order[batch*batch_size:(batch+1)*batch_size])\n",
    "print(tot_loss/tot_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002267783274874091"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S,V,D = np.linalg.svd(embeddings.detach().cpu(),full_matrices=False)\n",
    "\n",
    "# plot \n",
    "N=list(np.arange(40)+1)\n",
    "\n",
    "loss=np.array([torch.mean(torch.square(torch.tensor((S[:,:n]*V[:n])@D[:n],device=embeddings.device)-embeddings)).item() for n in N])\n",
    "loss[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# clone the model to not modify the original\n",
    "apx_model = make_model('prajjwal1/bert-tiny',args)\n",
    "apx_model.load_state_dict(model.state_dict())\n",
    "\n",
    "apxSVD = ApxSVD(apx_model.bert.embeddings.word_embeddings.weight, 5)\n",
    "apx_model.bert.embeddings.word_embeddings = apxSVD.to('cuda')\n",
    "\n",
    "val_metrics = validate(apx_model, tokenizer, val_dataloader, metrics, args)\n",
    "results['SVD']=[m.item() for m in val_metrics]\n",
    "print('SVD', val_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cola\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(task)\n\u001b[0;32m      5\u001b[0m args\u001b[39m=\u001b[39mGLUE_TASKS[task]\n\u001b[1;32m----> 6\u001b[0m model\u001b[39m=\u001b[39mmake_model(\u001b[39m'\u001b[39;49m\u001b[39mprajjwal1/bert-tiny\u001b[39;49m\u001b[39m'\u001b[39;49m,args)\n\u001b[0;32m      8\u001b[0m train_dataloader, val_dataloader \u001b[39m=\u001b[39m get_dataloaders(args, task, batch_size)\n\u001b[0;32m      9\u001b[0m metrics \u001b[39m=\u001b[39m get_metrics(args)\n",
      "File \u001b[1;32mc:\\Sam\\Scuola\\02_UniBo\\2_anno\\NLP\\CodeBookEmbeddings\\glue_score.py:28\u001b[0m, in \u001b[0;36mmake_model\u001b[1;34m(model_name, args, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m         model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_classes\u001b[39m\u001b[39m'\u001b[39m], ignore_mismatched_sizes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 28\u001b[0m         model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m     29\u001b[0m \u001b[39melif\u001b[39;00m args[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     30\u001b[0m     model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(model_name, num_labels\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, ignore_mismatched_sizes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    462\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[0;32m    464\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    465\u001b[0m     )\n\u001b[0;32m    466\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    467\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\modeling_utils.py:2276\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2273\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[0;32m   2275\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m-> 2276\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[0;32m   2278\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit:\n\u001b[0;32m   2279\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbitsandbytes\u001b[39;00m \u001b[39mimport\u001b[39;00m get_keys_to_not_convert, replace_8bit_linear\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1526\u001b[0m, in \u001b[0;36mBertForSequenceClassification.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m   1523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mnum_labels\n\u001b[0;32m   1524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m-> 1526\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m BertModel(config)\n\u001b[0;32m   1527\u001b[0m classifier_dropout \u001b[39m=\u001b[39m (\n\u001b[0;32m   1528\u001b[0m     config\u001b[39m.\u001b[39mclassifier_dropout \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mclassifier_dropout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m config\u001b[39m.\u001b[39mhidden_dropout_prob\n\u001b[0;32m   1529\u001b[0m )\n\u001b[0;32m   1530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDropout(classifier_dropout)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:891\u001b[0m, in \u001b[0;36mBertModel.__init__\u001b[1;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[0;32m    888\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[0;32m    890\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings \u001b[39m=\u001b[39m BertEmbeddings(config)\n\u001b[1;32m--> 891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m BertEncoder(config)\n\u001b[0;32m    893\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39m=\u001b[39m BertPooler(config) \u001b[39mif\u001b[39;00m add_pooling_layer \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[39m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:559\u001b[0m, in \u001b[0;36mBertEncoder.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m--> 559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:559\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig \u001b[39m=\u001b[39m config\n\u001b[1;32m--> 559\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([BertLayer(config) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config\u001b[39m.\u001b[39mnum_hidden_layers)])\n\u001b[0;32m    560\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient_checkpointing \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:474\u001b[0m, in \u001b[0;36mBertLayer.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_size_feed_forward \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mchunk_size_feed_forward\n\u001b[0;32m    473\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseq_len_dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 474\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m BertAttention(config)\n\u001b[0;32m    475\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_decoder \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mis_decoder\n\u001b[0;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_cross_attention \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39madd_cross_attention\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:394\u001b[0m, in \u001b[0;36mBertAttention.__init__\u001b[1;34m(self, config, position_embedding_type)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config, position_embedding_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    393\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself \u001b[39m=\u001b[39m BertSelfAttention(config, position_embedding_type\u001b[39m=\u001b[39;49mposition_embedding_type)\n\u001b[0;32m    395\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m BertSelfOutput(config)\n\u001b[0;32m    396\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpruned_heads \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:256\u001b[0m, in \u001b[0;36mBertSelfAttention.__init__\u001b[1;34m(self, config, position_embedding_type)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(config\u001b[39m.\u001b[39mhidden_size \u001b[39m/\u001b[39m config\u001b[39m.\u001b[39mnum_attention_heads)\n\u001b[0;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size\n\u001b[1;32m--> 256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(config\u001b[39m.\u001b[39;49mhidden_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_head_size)\n\u001b[0;32m    257\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size)\n\u001b[0;32m    258\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(config\u001b[39m.\u001b[39mhidden_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_head_size)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:101\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister_parameter(\u001b[39m'\u001b[39m\u001b[39mbias\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_parameters()\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:107\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreset_parameters\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     \u001b[39m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     init\u001b[39m.\u001b[39;49mkaiming_uniform_(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, a\u001b[39m=\u001b[39;49mmath\u001b[39m.\u001b[39;49msqrt(\u001b[39m5\u001b[39;49m))\n\u001b[0;32m    108\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         fan_in, _ \u001b[39m=\u001b[39m init\u001b[39m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\miniconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\init.py:412\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[0;32m    410\u001b[0m bound \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39m3.0\u001b[39m) \u001b[39m*\u001b[39m std  \u001b[39m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49muniform_(\u001b[39m-\u001b[39;49mbound, bound)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "RESULTS={}\n",
    "for task in list(GLUE_TASKS.keys()):\n",
    "    \n",
    "    print(task)\n",
    "    args=GLUE_TASKS[task]\n",
    "    model=make_model('prajjwal1/bert-tiny',args)\n",
    "    \n",
    "    train_dataloader, val_dataloader = get_dataloaders(args, task, batch_size)\n",
    "    metrics = get_metrics(args)\n",
    "\n",
    "    Glue(model, tokenizer, task, args, epochs=epochs, steps_validate=0.2, train_dataloader=train_dataloader, val_dataloader=val_dataloader);\n",
    "\n",
    "    embeddings = model.bert.embeddings.word_embeddings.weight.to(device)\n",
    "\n",
    "    results={}\n",
    "\n",
    "    val_metrics = validate(model, tokenizer, val_dataloader, metrics, args)\n",
    "    results['Original']=[m.item() for m in val_metrics]\n",
    "    print('Original', val_metrics)\n",
    "\n",
    "    for norm in norms:\n",
    "\n",
    "        # clone the model to not modify the original\n",
    "        apx_model = make_model('prajjwal1/bert-tiny',args)\n",
    "        apx_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        apx = ApproxEmbed(levels = levels, \n",
    "                feature_dim = channels,\n",
    "                num_words = embeddings.shape[0],\n",
    "                output_dims = embeddings.shape[1],\n",
    "                feature_std = 0.1,\n",
    "                feature_bias = 0.0,\n",
    "                codebook_bitwidth=bits,\n",
    "                neurons = neurons,\n",
    "                nn_levels=nn_levels).to('cuda')\n",
    "\n",
    "        train_apx(apx, embeddings, apx_epochs, apx_batch_size, checkpoint_every, save_path);\n",
    "        apx.fix_indices()\n",
    "        apx_model.bert.embeddings.word_embeddings = apx\n",
    "\n",
    "        val_metrics = validate(apx_model, tokenizer, val_dataloader, metrics, args)\n",
    "        results[\"Norm \"+str(norm)]=[m.item() for m in val_metrics]\n",
    "        print(norm, val_metrics)\n",
    "\n",
    "    # clone the model to not modify the original\n",
    "    apx_model = make_model('prajjwal1/bert-tiny',args)\n",
    "    apx_model.load_state_dict(model.state_dict())\n",
    "\n",
    "    apxSVD = ApxSVD(apx_model.bert.embeddings.word_embeddings.weight, 5)\n",
    "    apx_model.bert.embeddings.word_embeddings = apxSVD.to('cuda')\n",
    "\n",
    "    val_metrics = validate(apx_model, tokenizer, val_dataloader, metrics, args)\n",
    "    results['SVD']=[m.item() for m in val_metrics]\n",
    "    print('SVD', val_metrics)\n",
    "\n",
    "    RESULTS[task]=results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cola': {'Original': [0], 'Norm1.25': [0], 'SVD': [0]}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path='results/'\n",
    "import os\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "torch.save(RESULTS, save_path+'glue_results_no_retrain.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cola</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Norm1.25</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVD</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cola\n",
       "Norm1.25  0.0\n",
       "Original  0.0\n",
       "SVD       0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "run_dict=torch.load('results/glue_results_no_retrain.pth')\n",
    "\n",
    "for k_runs, runs in run_dict.items():\n",
    "    for k_run, run in runs.items():\n",
    "        run_dict[k_runs][k_run]=f\"{run[0]*100:,.1f}\" if len(run)==1 else f\"{run[0]*100:,.1f}/{run[1]*100:,.1f}\"\n",
    "df=pd.DataFrame(run_dict)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "506daaeaa79bc5d30715519e2bba71fd2fb898b1f12d903345e89400e3b4f753"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
